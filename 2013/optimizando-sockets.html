<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)

	Modified by Alex Fern√°ndez
	https://pinchito.es/ | @pinchito
-->
<html>
	<head>
		<title>Optimizando sockets con node.js</title>
		<meta charset="utf-8" />
		<meta name="description" content="Optimizando sockets con node.js ‚Äî " />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="twitter:card" content="summary" />
		<meta name="twitter:site" content="@pinchito" />
		<meta name="twitter:title" content="Optimizando sockets con
node.js ‚Äî " />
		<meta name="twitter:description" content="" />
		<meta name="twitter:image" content="" />
		<meta property="og:title" content="Optimizando sockets con
node.js ‚Äî " />
		<meta property="og:type" content="website" />
		<meta property="og:description" content="" />
		<meta property="og:image" content="" />
		<meta property="og:url" content="https://pinchito.es/2013/optimizando-sockets" />
		<link rel="stylesheet" href="/css/main.css" />
		<link rel="canonical" href="https://pinchito.es/2013/optimizando-sockets" />
		<link rel="shortcut icon" href="/favicon.png" type="image/png" />
		<!--[if lte IE 8]><link rel="stylesheet" href="/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Header -->
			<section id="header">
				<header>
					<p class="home"><a class="home" href="/">pinchito.es</a></p>
					<p>
					<a href="/about">about</a>
					<br/>
					<a href="/cv">CV</a>
					<br/>
					<a href="/rss.xml">
						<svg aria-hidden="true" class="rss-icon" width="18" height="18" viewBox="0 0 18 18"><path d="M3 1a2 2 0 0 0-2 2v12c0 1.1.9 2 2 2h12a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2zm0 1.5c6.9 0 12.5 5.6 12.5 12.5H13C13 9.55 8.45 5 3 5zm0 5c4.09 0 7.5 3.41 7.5 7.5H8c0-2.72-2.28-5-5-5zm0 5c1.36 0 2.5 1.14 2.5 2.5H3z"></path></svg>
						RSS feed</a>
					</p>
					<p>
					<a id="follower" href="https://twitter.com/intent/follow?screen_name=pinchito" target="_blank"><i id="birdie"></i>@pinchito</a>
					<br />
					<a rel="me" href="https://mastodon.social/@pinchito" target="_blank">üêò @pinchito@mastodon.social</a>
					<br/>
					<a href="https://github.com/alexfernandez" aria-label="Follow @alexfernandez on GitHub">alexfernandez @ GitHub</a>
					</p>
					<p>
					This site is cookie free! No tracking is done on your browser.
					</p>
					<a href="https://librecounter.org/referer/show" target="_blank">
					<img src="https://librecounter.org/counter.svg" referrerPolicy="unsafe-url">
					</a>
				</header>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<section id="zero">
							<p class="home"><a class="home" href="/">pinchito.es</a></p>
						</section>

						<!-- One -->
							<section id="one">
								<div class="container">
									<header class="major">
										<h1>Optimizando sockets con node.js</h1>
										<p><p>
									</header>
																				<figure>
<img src="pics/optimizando-sockets-portada.jpg"
title="Gr√°fica de rendimiento" alt="Cr√©dito: Alex Fern√°ndez" />
<figcaption aria-hidden="true">Cr√©dito: Alex Fern√°ndez</figcaption>
</figure>
<p>Este post es una especie de segunda parte de <a
href="nodejs-rapido-como-el-rayo">Node.js: ¬ør√°pido como el rayo?</a>. En
esta ocasi√≥n vamos a intentar superarnos para mejorar el rendimiento de
nuestro servidor nodecached, un proyecto para hacer un clon de memcached
en node.js.</p>
<h2 id="punto-de-partida">Punto de partida</h2>
<p>Nuestro punto de partida es muy sencillo: el <a
href="http://www.memcached.org/">memcached original</a> nos da m√°s de 70
mil peticiones por segundo (70 krps, por <em>kilo requests per
second</em>), mientras que nodecached dif√≠cilmente pasa de 30k. ¬øEs
posible mejorar esta situaci√≥n? Vamos a intentarlo.</p>
<p>La funci√≥n a optimizar es <a
href="https://github.com/alexfernandez/nodecached/blob/master/lib/server.js#L141"><code>readData()</code></a>
en la clase <code>Connection</code>, que citamos aqu√≠ para
referencia:</p>
<pre><code>    function readData(data)
    {
        [...]
        var message = data.toString();
        var line = message.substringUpTo(&#39;\r\n&#39;);
        var rest = message.substringFrom(&#39;\r\n&#39;);
        var result = parser.readLine(line);
        if (rest)
        {   
            [...]
            result = parser.readLine(rest);
        }
        [...]
        if (result)
        {
            self.socket.write(result + &#39;\r\n&#39;);
        }   
    }</code></pre>
<p>Es aqu√≠ donde vamos a pasar todo el tiempo de este post. Quitando el
manejo de algunas opciones, lo que hace esta funci√≥n es recibir datos
del socket, convertirlos en cadena, partirlos en l√≠neas y parsearlos. El
resultado del parseo se escribe en el socket y se termina. Es un c√≥digo
s√≠ncrono muy f√°cil de seguir.</p>
<h3 id="granularidad">Granularidad</h3>
<p>Vamos a ver primero de qu√© tiempos estamos hablando. Nuesta √∫ltima
prueba exitosa con nodecached, usando <a
href="https://github.com/antirez/mc-benchmark">mc-benchmark</a>, nos dio
estos resultados:</p>
<pre><code>$ ./mc-benchmark -p 11311 -c 100 -n 100000
====== SET ======
  100000 requests completed in 3.82 seconds
  100 parallel clients
  3 bytes payload
  keep alive: 1

[...]
26205.45 requests per second

====== GET ======
  100000 requests completed in 2.99 seconds
  100 parallel clients
  3 bytes payload
  keep alive: 1

[...]
33467.20 requests per second</code></pre>
<p>Si somos capaces de responder a 26.2 krps, quiere decir que cada
petici√≥n viene a tardar 1s/26.2k ~ 38 ¬µs. 38 microsegundos suena bien,
¬øno?. Para los get es a√∫n menos: 1s/33.5k ~ 0.000029851 s ~ 30¬µs, o sea
¬°unos 30 microsegundos!</p>
<p>Pero esto no es nada comparado con memcached:</p>
<pre><code>$ ./mc-benchmark -p 11215 -c 50 -n 100000
====== SET ======
  100016 requests completed in 1.19 seconds
  50 parallel clients
  3 bytes payload
  keep alive: 1

[...]
83906.04 requests per second

====== GET ======
  100012 requests completed in 0.95 seconds
  50 parallel clients
  3 bytes payload
  keep alive: 1

[...]
105054.62 requests per second</code></pre>
<p>Por alg√∫n motivo los resultados son ahora escandalosamente mejores
que los que obtuvimos en <a
href="http://www.godtic.com/blog/2013/11/12/node-js-rapido-como-el-rayo/">la
√∫ltima ocasi√≥n</a>, lo cual nos debe llevar a una desconfianza sana
hacia los resultados que podamos obtener ahora, por buenos o malos que
sean. En cualquier caso, los tiempos son ahora mucho mejores: 1/105k ~
9.5 ¬µs, o sea menos de 10 microsegundos. Y un pel√≠n m√°s para get, unos
12 ¬µs.</p>
<p>La situaci√≥n es la de la siguiente gr√°fica: tenemos un mont√≥n de
‚Äúespacio de mejora‚Äù por delante.</p>
<figure>
<img src="pics/optimizando-sockets-antes.png"
title="Gr√°fica de rendimiento inicial" alt="Rendimiento inicial" />
<figcaption aria-hidden="true">Rendimiento inicial</figcaption>
</figure>
<p>Una curiosidad: node.js no colapsa las dos CPUs de mi m√°quina,
siempre deja parte de la CPU disponible para otras tareas.</p>
<figure>
<img src="pics/optimizando-sockets-cpu-node.png"
title="CPUs con node.js" alt="CPUs con Node.js" />
<figcaption aria-hidden="true">CPUs con Node.js</figcaption>
</figure>
<p>Mientras que memcached es mucho m√°s glot√≥n y satura completamente las
dos CPUs.</p>
<figure>
<img src="pics/optimizando-sockets-cpu-memcached.png"
title="CPUs con memcached" alt="CPUs con memcached" />
<figcaption aria-hidden="true">CPUs con memcached</figcaption>
</figure>
<h3 id="buffers-y-strings">Buffers y Strings</h3>
<p>Uno de los consejos que aparecen bastante al optimizar node.js es
evitar las conversiones de <a
href="http://nodejs.org/api/buffer.html">Buffer</a> a <a
href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String">String</a>,
ya que pueden llevar mucho tiempo. ¬øCu√°nto exactamente? Vamos a
verlo.</p>
<p>En nuestro c√≥digo tenemos la siguiente l√≠nea en <a
href="https://github.com/alexfernandez/nodecached/blob/master/lib/server.js"><code>lib/server.js</code></a>
(en negrita):</p>
<pre><code>    function readData(data)
    {
        [...]
        var message = String(data);
        var line = message.substringUpTo(&#39;\r\n&#39;);
        [...]
    }</code></pre>
<p>Vamos a usar el temporizador de alta resoluci√≥n <a
href="http://nodejs.org/api/process.html#process_process_hrtime"><code>process.hrtime()</code></a>
para ver el tiempo exacto que tarda esta operaci√≥n:</p>
<pre><code>        var us = process.hrtime()[1];
        var message = String(data);
        var diff = process.hrtime()[1] - us;
        console.log(diff);</code></pre>
<p>Medimos el tiempo antes y despu√©s, calculamos la diferencia y la
mostramos en una traza. Al correr este c√≥digo en el servidor vemos una
serie interminable de n√∫meros, la mayor√≠a de los cuales ronda el
4000:</p>
<pre><code>4106
4114
4036
3988
3984
4029
3980
4394
4408
4404
4239
6351</code></pre>
<p>Al final vemos un <em>outlier</em> de 6351, y si dejamos el c√≥digo
corriendo un rato mientras lanzamos peticiones con mc-benchmark podemos
llegar a ver alg√∫n 896247. Dado que <code>process.hrtimer()</code>
devuelve un valor en nanosegundos (ns), cada mil de √©stos son un
microsegundo. As√≠ que ahora mismo estamos gastando 4 ¬µs (de nuestro
presupuesto de 30) en hacer la conversi√≥n, lo que es significativo (un
13%).</p>
<p>Una optimizaci√≥n obvia es usar la funci√≥n Buffer.toString().</p>
<pre><code>        var us = process.hrtime()[1];
        var message = data.toString();
        var diff = process.hrtime()[1] - us;
        console.log(diff);</code></pre>
<p>Con esta nueva versi√≥n la traza baja un poco:</p>
<pre><code>2302
6249
601792
6994
2452
2338
2333
2316
2208
2258
2395</code></pre>
<p>De nuevo vemos un <em>outlier</em> brutal, de 0.6 ms, pero la mayor√≠a
ronda los 2.3 ¬µs. ¬øSe nota diferencia realmente con esta nueva versi√≥n?
Veamos las estad√≠sticas de mc-benchmark:</p>
<pre><code>chenno@s110: ~/install/mc-benchmark $ ./mc-benchmark -p 11311 -c 100 -n 100000
====== SET ======
  100000 requests completed in 3.60 seconds
[...]
27785.50 requests per second

====== GET ======
  100000 requests completed in 2.83 seconds
[...]
35335.69 requests per second</code></pre>
<p>Ahora hemos subido a 35 krps, lo que nos da un tiempo de 28.6 ¬µs.
Como podr√≠amos esperar, nos hemos ahorrado casi 2 ¬µs de procesamiento.
De paso, hemos podido comprobar que las medidas de tiempo tomadas dentro
de node.js son congruentes con las que tomamos desde un cliente. ¬°Vamos
por el buen camino!</p>
<h2 id="optimizando-el-c√≥digo">Optimizando el c√≥digo</h2>
<p>Seguramente habr√©is o√≠do la famosa frase de <a
href="http://c2.com/cgi/wiki?PrematureOptimization">Donald
Knuth</a>:</p>
<blockquote>
<p>We should forget about small efficiencies, say about 97% of the time:
premature optimization is the root of all evil. Yet we should not pass
up our opportunities in that critical 3%.</p>
</blockquote>
<p>En este punto no parece prematuro optimizar nuestro c√≥digo, aunque la
funcionalidad de nodecached no est√© completa: ya tenemos un <a
href="http://www.godtic.com/blog/2013/08/27/pruebas-de-carga/">caso de
uso definido y concreto</a> que podemos medir. S√≥lo necesitamos una cosa
para optimizar con garant√≠as: tener medidas precisas de tiempos, para
poder compararlas antes y despu√©s de cada modificaci√≥n y ver si
realmente ganamos algo.</p>
<h3 id="marcado-y-perfilado">Marcado y perfilado</h3>
<p>Una vez verificado que el benchmark externo da resultados parecidos a
la medici√≥n interna, podemos refinar el c√≥digo de medici√≥n para que haga
algo m√°s que pintar una ristra de n√∫meros. Lo que necesitamos es un
perfilador, o <em>profiler</em>: algo que mida el tiempo que requieren
una o m√°s operaciones.</p>
<p>Vamos a usar el siguiente c√≥digo:</p>
<pre><code>/**
 * Profile some piece of code.
 */
function Profiler()
{
    // self-reference
    var self = this;

    // attributes
    var requests = 0;
    var time = 0;

    /**
     * Take a measurement between before and after, show results every few requests.
     */ 
    self.measure = function(before, after)
    {
        requests += 1;
        var diff = after[1] - before[1];
        if (after[0] &gt; before[0])
        {
            diff += 1000000000;
        }
        time += diff;
        if (requests % 100000 === 0)
        {
            console.log(&#39;Requests: %s, mean time: %s ¬µs&#39;, requests, time / requests / 1000);
            requests = 0;
            time = 0;
        }
    };
}
var profiler = new Profiler();</code></pre>
<p>Lo primero que hace es crear el n√∫mero de peticiones
<code>requests</code> y el tiempo total <code>time</code>, y luego con
cada llamada a <code>measure(before, after)</code> obtiene la diferencia
de tiempos entre ambas mediciones y lo va acumulando. Cada 100k
peticiones se pinta la media de tiempos en ¬µs y se resetean los valores.
El c√≥digo es un pel√≠n enrevesado porque <code>process.hrtimer()</code>
devuelve una estructura con [segundos, nanosegundos] que hay que
interpretar, pero todo el resto es trivial.</p>
<p>Observa c√≥mo creamos un solo <code>profiler</code> para todo el
c√≥digo, de forma que diferentes peticiones se acumulen en un √∫nico
lugar.</p>
<p>Para usarlo s√≥lo tenemos que obtener el tiempo antes y despu√©s y
llamar a <code>profiler.measure(before, after)</code>.</p>
<pre><code>        var us = process.hrtime();
        [...]
        profiler.measure(us, process.hrtime());</code></pre>
<p>Podemos arrancar el servidor y lanzarle 100k peticiones con
mc-benchmark para acumular suficientes medidas:</p>
<pre><code>$ ./mc-benchmark -p 11311 -n 100000</code></pre>
<p>Es posible que tengamos que probar varias veces para conseguir un
valor estable, pero con 100k peticiones no hay problema en quedarnos con
el valor m√°s peque√±o que veamos salvo que sea un <em>outlier</em> (est√©
muy lejos del resto). Primero vamos a obtener una base de medida (lo que
llaman los americanos una <em>baseline</em>, aunque la pronunciaci√≥n en
espa√±ol sea desafortunada):</p>
<pre><code>        var us = process.hrtime();
        profiler.measure(us, process.hrtime());</code></pre>
<p>Es decir, lo que tarda la propia medida. Mis resultados son bastante
consistentes:</p>
<pre><code>Requests: 100000, mean time: 1.03806404 ¬µs
Requests: 100000, mean time: 1.06449447 ¬µs</code></pre>
<p>Alrededor de un microsegundo tarda el propio c√≥digo de medici√≥n. Por
lo tanto tendremos que acordarnos siempre de quitar este microsegundo de
los valores que midamos.</p>
<h3 id="perfilando-la-conversi√≥n-de-buffer">Perfilando la conversi√≥n de
Buffer</h3>
<p>Ahora ya podemos meter el c√≥digo que queramos entre el primero
<code>process.hrtime()</code> y el segundo, y vamos a empezar con las
conversiones de Buffer a String que hemos estado mirando:</p>
<pre><code>        var us = process.hrtime();
        var message = data.toString(&#39;utf8&#39;);
        profiler.measure(us, process.hrtime());</code></pre>
<p>El resultado anda alrededor de los 2.25 ¬µs, consistente con nuestras
mediciones anteriores m√°s anecd√≥ticas.</p>
<pre><code>Requests: 100000, mean time: 2.2132430199999997 ¬µs
Requests: 100000, mean time: 2.08822848 ¬µs
Requests: 100000, mean time: 2.3663381500000003 ¬µs
Requests: 100000, mean time: 2.168189 ¬µs</code></pre>
<p>Quitando el microsegundo de la medida obtenemos cerca de 1.20 ¬µs para
el set y ~1 ¬µs para el get.</p>
<p>Podemos ponernos creativos y probar diferentes cosas, a ver si
ahorramos algunos microsegundos m√°s. Recuerda que nuestro presupuesto
total eran 30 ¬µs y el de memcached 10, as√≠ que cada microsegundo que
ahorremos nos pone m√°s cerca de nuestro objetivo. ¬øQu√© pasa si hacemos
la conversi√≥n de Buffer a String usando el venerable ASCII? Es posible
que nuestro c√≥digo no entienda bien claves ni valores Unicode, pero
puede ser que eso no sea un problema en algunas situaciones.</p>
<pre><code>        var us = process.hrtime();
        var message = data.toString(&#39;ascii&#39;);
        profiler.measure(us, process.hrtime());</code></pre>
<p>El resultado est√° ahora cerca de 2.08 ¬µs (1 ¬µs quitando el tiempo de
medida). No parece que rasquemos mucho, y s√≠ perdemos bastante
funcionalidad.</p>
<p>¬øQu√© ocurre si volvemos al c√≥digo original?</p>
<pre><code>        var us = process.hrtime();
        var message = String(data);
        profiler.measure(us, process.hrtime());</code></pre>
<p>¬°Sorpresa! Aunque el c√≥digo es virtualmente id√©ntico que en la prueba
anterior, obtenemos tiempos mucho mejores que antes (aunque siguen
siendo algo peores que con <code>toString()</code>):</p>
<pre><code>Requests: 100000, mean time: 2.77426338 ¬µs
Requests: 100000, mean time: 2.50233279 ¬µs
Requests: 100000, mean time: 2.55807264 ¬µs
Requests: 100000, mean time: 2.38827297 ¬µs</code></pre>
<p>Incluso con el microsegundo de medida, la medici√≥n est√° entre entre
2.8 y 2.4 ¬µs, muy lejos de los 4 ¬µs que ten√≠amos antes. ¬øPor qu√© esta
fluctuaci√≥n con el mismo c√≥digo? De nuevo un resultado que nos hace
desconfiar de todas las medidas que tomemos, sobre todo para tiempos
peque√±os.</p>
<h3 id="perfilando-el-resto">Perfilando el resto</h3>
<p>Aprovechando esta infraestructura podemos ver lo que tarda el
procesamiento de los datos en nodecached:</p>
<pre><code>        var us = process.hrtime();
        var line = message.substringUpTo(&#39;\r\n&#39;);
        var rest = message.substringFrom(&#39;\r\n&#39;);
        var result = parser.readLine(line);
        if (rest)
        {   
            if (result)
            {
                log.error(&#39;Unexpected result %s, ignoring&#39;, result);
            }
            result = parser.readLine(rest);
        }
        profiler.measure(us, process.hrtime());</code></pre>
<p>Dado que mc-benchmark lanza 100k sets y 100k gets, podemos incluso
ver la diferencia de tiempos entre ambas operaciones en dos benchmarks
consecutivos:</p>
<pre><code>Requests: 100000, mean time: 13.74600158 ¬µs
Requests: 100000, mean time: 8.08778811 ¬µs
Requests: 100000, mean time: 14.07496895 ¬µs
Requests: 100000, mean time: 8.13666839 ¬µs</code></pre>
<p>Quitando el microsegundo de medida, los sets tienen que procesar m√°s
datos y tardan alrededor de 13 ¬µs, mientras que los gets est√°n alrededor
de 7 ¬µs.</p>
<h3 id="subcadenas">Subcadenas</h3>
<p>Podemos afinar un poco m√°s: ¬øcu√°nto tardan las operaciones de cadena
del principio?</p>
<pre><code>        var us = process.hrtime();
        var line = message.substringUpTo(&#39;\r\n&#39;);
        var rest = message.substringFrom(&#39;\r\n&#39;);
        profiler.measure(us, process.hrtime());</code></pre>
<p>Sorprendentemente, bastante:</p>
<pre><code>Requests: 100000, mean time: 2.5679964799999997 ¬µs
Requests: 100000, mean time: 2.19254304 ¬µs
Requests: 100000, mean time: 2.44509724 ¬µs
Requests: 100000, mean time: 2.2384055800000002 ¬µs</code></pre>
<p>Un poco m√°s en los sets que en los gets, posiblemente porque tienen
m√°s datos. Quitando el ¬µs de medida, tenemos 1~1,5¬µs de procesamiento;
¬øno parece mucho para un par de <code>substring()</code>s?</p>
<p>Vamos a intentar reducir este tiempo usando manipulaciones del buffer
directamente:</p>
<pre><code>        var us = process.hrtime();
        var rn = 0;
        for (var i = 0; i &lt; data.length; i++)
        {
            if (data[i] == 13)
            {
                rn = i;
                break;
            }
        }
        var line, rest;
        if (rn)
        {
            line = data.toString(&#39;utf8&#39;, 0, rn);
            rest = data.toString(&#39;utf8&#39;, rn + 2);
        }
        else
        {
            line = data.toString(&#39;utf8&#39;);
        }
        profiler.measure(us, process.hrtime());</code></pre>
<p>Este c√≥digo reemplaza no s√≥lo a las subcadenas, sino tambi√©n a la
conversi√≥n de Buffer a String del principio. B√°sicamente buscamos un
lugar del buffer que contenga un salto de l√≠nea, y una vez encontrado
hacemos dos <code>toString()</code> para extraer dos subcadenas
directamente. Los resultados son prometedores, pero no muy diferentes de
los que ten√≠amos antes:</p>
<pre><code>Requests: 100000, mean time: 3.1032178 ¬µs
Requests: 100000, mean time: 2.84020157 ¬µs</code></pre>
<p>Quitando el ¬µs de medida, ser√≠an 1.8~2.1 ¬µs ahora, contra 1+1.5=2.5
¬µs antes. Un ahorro bastante modesto que no est√° claro que compense la
complejidad adicional del c√≥digo, pero algo es algo. Adem√°s dejamos el
procesamiento interno en 6 ¬µs, quitando las transformaciones de cadena
iniciales.</p>
<h3 id="escritura-de-datos">Escritura de datos</h3>
<p>Ya s√≥lo nos falta un elemento: ¬øcu√°nto tarda la escritura de los
datos?</p>
<pre><code>        var us = process.hrtime();
        if (result)
        {
            self.socket.write(result + &#39;\r\n&#39;);
        }
        profiler.measure(us, process.hrtime());</code></pre>
<p>Los resultados son m√°s homog√©neos que antes, pero tambi√©n muestran
cierta diferencia de tiempo entre escribir al socket la respuesta a un
set y la de un get.</p>
<pre><code>Requests: 100000, mean time: 14.778381139999999 ¬µs
Requests: 100000, mean time: 14.03921801 ¬µs
Requests: 100000, mean time: 14.95215595 ¬µs
Requests: 100000, mean time: 14.10403694 ¬µs</code></pre>
<p>Resulta bastante curioso que tarde m√°s la respuesta al get que al
set, ya que el get tiene que devolver datos; hasta que nos damos cuenta
de que el set devuelve <code>STORED</code> y el get no tiene datos y
simplemente tiene que escribir <code>END</code>.</p>
<p>Hemos visto c√≥mo optimizar la conversi√≥n de entrada de Buffer a
String, pero ¬øqu√© ocurre con la conversi√≥n contraria? Seguramente haya
una conversi√≥n a Buffer impl√≠cita en la escritura. Para ver si podemos
ahorrarnos algo de tiempo, probamos a enviar un buffer constante:</p>
<pre><code>        var us = process.hrtime();
        if (result)
        {
            self.socket.write(ERROR_BUFFER);
        }
        profiler.measure(us, process.hrtime());</code></pre>
<p>Si con esto no reducimos tiempos es que realmente da igual pasar un
String o un Buffer. Y la pr√°ctica respalda esta teor√≠a: los resultados
son pr√°cticamente iguales a los anteriores, 14.9 ¬µs para get y 13.9 para
set, lo que indica que no ahorramos nada. Aqu√≠ s√≠ que no hay de donde
rascar: la escritura es realmente en un buffer interno, lo que deber√≠a
ser casi instant√°neo. Sin embargo, por alg√∫n motivo tarda m√°s que el
memcached original en procesar toda la petici√≥n y enviar la
respuesta.</p>
<h3 id="todos-juntos">Todos juntos</h3>
<p>Tras nuestras mejoras obtenemos estos resultados con nodecached:</p>
<pre><code>====== SET ======
  100000 requests completed in 3.55 seconds
  [...]
28208.74 requests per second

====== GET ======
  100000 requests completed in 2.68 seconds
  [...]
37243.95 requests per second</code></pre>
<p>Hemos reducido el tiempo del get a 1s/37k ~ 27 ¬µs, y el set a 35.4
¬µs. La siguiente figura muestra el desglose de tiempos en la nueva
distribuci√≥n.</p>
<figure>
<img src="pics/optimizando-sockets-despues.png"
title="Gr√°fica de rendimiento final" alt="Rendimiento final" />
<figcaption aria-hidden="true">Rendimiento final</figcaption>
</figure>
<p>Las √°reas claras del gr√°fico son las que est√°n directamente bajo
nuestro control; se ve claramente que son minor√≠a. Vamos a centrarnos en
la petici√≥n get, por ejemplo. Del total de 27 ¬µs, 2 ¬µs son para
conversi√≥n a string y divisi√≥n en l√≠neas; otros 6 ¬µs para el
procesamiento interno, y 12 ¬µs para escribir la respuesta. Pero estos
tiempos desglosados suman 20 ¬µs; ¬ød√≥nde est√°n los 7 ¬µs restantes?
Podemos suponer que es el tiempo interno que gasta node.js en recibir
una petici√≥n y montar las estructuras de datos necesarias para responder
a ella.</p>
<p>Si hacemos las cuentas para el set, vemos que este tiempo ‚Äúfantasma‚Äù
son 8 ¬µs: 35-2-12-13. Tiene sentido que sea un poco mayor porque
recibimos m√°s datos en el set.</p>
<h3 id="otros-perfiladores">Otros perfiladores</h3>
<p>Es probable que hayas pensado: ¬øpor qu√© usar un perfilador
cutre-salchichero hecho a mano, teniendo tantos profilers maravillosos
para node.js? Pues pi√©nsatelo dos veces. Nuestro perfilador est√°
altamente optimizado para nuestra situaci√≥n particular, tiene una
penalizaci√≥n f√°cilmente medible (1 ¬µs por petici√≥n) y nos ha dado datos
que hemos podido contrastar directamente con nuestro benchmark externo.
Ahora pregunto yo: ¬øpor qu√© complicarse la vida con un paquete escrito
en C, que lee directamente los datos de V8 o lo que sea, que tiene una
penalizaci√≥n de tiempo desconocida, cuando nuestro propio perfilador es
trivial y sabemos c√≥mo funciona? Francamente, no creo que las
oscilaciones de resultados que estamos viendo fueran sustancialmente
menores con otros perfiladores.</p>
<p>En cualquier caso, es buena idea usar un perfilador externo para
contrastar nuestros datos. Pero no vamos a hacerlo aqu√≠ porque bastante
larga es ya esta entrada como para meter m√°s datos redundantes‚Ä¶</p>
<h2 id="conclusiones">Conclusiones</h2>
<p>Es hora de sacar conclusiones de nuestro trabajo de optimizaci√≥n.</p>
<ul>
<li>Aparte de algunas mejoras r√°pidas y poco significativas, <em>no hay
balas de plata</em> que nos permitan optimizar el c√≥digo actual con poco
trabajo.</li>
<li>Dicho esto, con bastante trabajo y ara√±ando de aqu√≠ y all√° hemos
rebajado nuestro presupuesto inicial para una petici√≥n get, de los 30 ¬µs
del principio del post a 27 ¬µs, a base de optimizar nuestro c√≥digo. Un
10% de mejora que no est√° mal.</li>
<li>Otro resultado del trabajo de optimizaci√≥n es que nos permite saber
en qu√© estamos gastando el tiempo, cu√°l es nuestro ‚Äúpresupuesto‚Äù de
tiempo a gastar, y qu√© partes est√°n bajo nuestro control y cu√°les
no.</li>
<li><em>La mayor parte del tiempo de proceso est√° fuera de nuestro
alcance</em>: de los 30 ¬µs por operaci√≥n, unos 20 (13 + 7) est√°n
dedicados a montar la petici√≥n y escribir la respuesta. Nuestro trabajo
de optimizaci√≥n est√° limitado a las restantes partes; y una aplicaci√≥n
trivial de la <a href="http://es.wikipedia.org/wiki/Ley_de_Amdahl">ley
de Amdahl</a> nos dice que nunca podremos llegar a rivalizar con
memcached, al menos en rendimiento.</li>
<li>Finalmente, <em>los resultados de estos benchmarks son bastante
ruidosos</em>: es posible que la m√°quina usada tenga bastante carga de
navegadores y procesos de background que no es f√°cil de ver con
<code>top</code> o herramientas similares. Pero eliminar todos los
procesos de una m√°quina de escritorio es casi imposible, y adem√°s la
m√°quina se volver√≠a bastante inusable‚Ä¶</li>
</ul>
								</div>
							</section>
							<section id="last">
								<div class="container">
									<p>
									Original publicado en <a
href="http://www.godtic.com/blog/2013/11/24/optimizando-sockets-con-node-js/">GodTIC</a>
el 2013-11-24.
									</p>
									<p>
									Back to the <a href="/">index</a>.
									</p>
								</div>
							</section>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>
									¬© <a href="mailto:alexfernandeznpm@gmail.com">Alex Fern√°ndez</a>.
									<a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>.
								</li>
								<li>Original design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</section>

			</div>
	</body>
</html>
