<!DOCTYPE HTML>
<!--
	Read Only by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)

	Modified by Alex Fern√°ndez
	https://pinchito.es/ | @pinchito
-->
<html>
	<head>
		<title>Pruebas de carga</title>
		<meta charset="utf-8" />
		<meta name="description" content="Pruebas de carga ‚Äî " />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<meta name="twitter:card" content="summary" />
		<meta name="twitter:site" content="@pinchito" />
		<meta name="twitter:title" content="Pruebas de carga ‚Äî " />
		<meta name="twitter:description" content="" />
		<meta name="twitter:image" content="" />
		<link rel="stylesheet" href="/css/main.css" />
		<link rel="canonical" href="https://pinchito.es/2013/pruebas-de-carga" />
		<link rel="shortcut icon" href="/favicon.png" type="image/png" />
		<!--[if lte IE 8]><link rel="stylesheet" href="/css/ie8.css" /><![endif]-->
		<script>
		// Disable tracking if the opt-out cookie exists.
		var disableStr = 'ga-disable-UA-75898530-1';
		if (document.cookie.indexOf(disableStr + '=true') > -1) {
			  window[disableStr] = true;
		}
		// Opt-out function
		function gaOptout() {
			document.cookie = disableStr + '=true; expires=Thu, 31 Dec 2099 23:59:59 UTC; path=/';
			window[disableStr] = true;
		}
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
		ga('create', 'UA-75898530-2', 'auto');
		ga('send', 'pageview');

		</script>
	</head>
	<body>

		<!-- Header -->
			<section id="header">
				<header>
					<p class="home"><a class="home" href="/">pinchito.es</a></p>
					<p>
					<a href="/about">about</a>
					<br/>
					<a href="/cv">CV</a>
					<br/>
					<a href="/speaker">speaker</a>
					</p>
					<p>
					<a id="follower" href="https://twitter.com/intent/follow?screen_name=pinchito" target="_blank"><i id="birdie"></i>@pinchito</a>
					<br />
					<a rel="me" href="https://mastodon.social/@pinchito" target="_blank">üêò @pinchito@mastodon.social</a>
					<br/>
					<a href="https://github.com/alexfernandez" aria-label="Follow @alexfernandez on GitHub">alexfernandez @ GitHub</a>
					<br/>
					<a href="https://www.youtube.com/channel/UCp5fMWhuqcbrvSJEOByeGwg">YouTube</a>
					</p>
					<p>
					This site uses (gasp!) cookies for gathering statistics.
					You can
						<a href="javascript:gaOptout()">disable them</a>.
					</p>
				</header>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">

						<section id="zero">
							<p class="home"><a class="home" href="/">pinchito.es</a></p>
						</section>

						<!-- One -->
							<section id="one">
								<div class="container">
									<header class="major">
										<h1>Pruebas de carga</h1>
										<p><p>
									</header>
																				<figure>
                    <img src="pics/pruebas-de-carga.jpg" title="Weighing cotton bales at the Brisbane ginnery" alt="" /><figcaption>Imagen: <a href="http://commons.wikimedia.org/wiki/File:StateLibQld_2_119360_Weighing_cotton_bales_at_the_Brisbane_ginnery.jpg">Queenslander</a></figcaption>
                    </figure>
                    <p>En esta ocasi√≥n nos vamos a alejar por un rato de node.js, y vamos a ver c√≥mo hacer pruebas de carga a nuestro servidor web.</p>
                    <h2 id="herramientas">Herramientas</h2>
                    <p>Para empezar vamos a mirar un par de herramientas muy sencillas y libres (o si lo prefieres open source) que podemos usar completamente gratis.</p>
                    <h3 id="apache-ab">Apache ab</h3>
                    <p>La m√°s conocida es <a href="http://httpd.apache.org/docs/2.2/programs/ab.html">ab</a>, por <em>Apache Benchmark</em>. Un ejemplo sencillo de uso:</p>
                    <pre><code>$ ab -c 10 -n 1000 http://127.0.0.1:8080/</code></pre>
                    <p>Los par√°metros m√°s importantes son:</p>
                    <ul>
                    <li>-c <em>concurrencia</em>: n√∫mero de peticiones concurrentes. Se lanzan tantos hilos como indique este par√°metro, as√≠ que nunca tendremos m√°s peticiones en vuelo que este n√∫mero.</li>
                    <li>-n <em>peticiones</em>: n√∫mero total de peticiones a lanzar.</li>
                    <li>-t <em>segundos</em>: tiempo de pruebas, tras el que ab dejar√° de recibir peticiones.</li>
                    </ul>
                    <p>Despu√©s de los par√°metros se a√±ade la URL a probar. Hay otras opciones para enviar peticiones POST, a√±adir contenido en un formulario, y dem√°s; pero para empezar vamos a suponer que nuestra aplicaci√≥n tiene una <a href="http://es.wikipedia.org/wiki/Representational_State_Transfer">interfaz Rest</a> y que por tanto se pueden hacer peticiones GET enviando toda la informaci√≥n necesaria en la URL.</p>
                    <p><em>Los consejos del abuelo cebolleta</em>: si tu aplicaci√≥n no es Rest, haz que lo sea. Ser√° mil veces m√°s sencilla de probar, porque s√≥lo necesitar√°s un navegador para depurarla. ¬°Haz tu vida m√°s sencilla!</p>
                    <p>Vamos a probar nuestro nuevo juguete contra un servidor que nos hemos montado con <a href="http://expressjs.com/">express</a>. La salida de ab ser√° algo as√≠ (dejando s√≥lo las l√≠neas m√°s importantes):</p>
                    <pre><code>$ ab -n 10000 http://127.0.0.1:8080/

                    Concurrency Level:      1
                    Time taken for tests:   4.895 seconds
                    Complete requests:      10000
                    Requests per second:    2042.84 [#/sec] (mean)
                    Time per request:       0.490 [ms] (mean)

                    Percentage of the requests served within a certain time (ms)
                      50%      0
                      66%      0
                      75%      0
                      80%      0
                      90%      1
                      95%      1
                      98%      1
                      99%      1
                     100%     11 (longest request)</code></pre>
                    <p>Los n√∫meros m√°s interesantes son las peticiones por segundo (abreviado req/s, por <em>requests per second</em>) y el tiempo por petici√≥n: estamos pasando de las 2000 req/s, y cada petici√≥n dura medio milisegundo, de media. Ahora vamos a a√±adir concurrencia con el par√°metro <code>-c</code>: en lugar de ir todas las peticiones en secuencia, una tras otra, lanzaremos varias a la vez. ¬øPara qu√© sirve tener m√∫ltiples peticiones a la vez? Para ver si nuestro servidor responde bien, claro: en la vida real un usuario no se espera a que termine el anterior para lanzar su petici√≥n. Adem√°s los resultados suelen ser incluso mejores:</p>
                    <pre><code>$ ab -c 10 -n 10000 http://127.0.0.1:8080/

                    Concurrency Level:      10
                    Time taken for tests:   3.334 seconds
                    Complete requests:      10000
                    Requests per second:    2999.08 [#/sec] (mean)
                    Time per request:       3.334 [ms] (mean)
                    Time per request:       0.333 [ms] (mean, across all concurrent requests)

                    Percentage of the requests served within a certain time (ms)
                      50%      3
                      66%      3
                      75%      3
                      80%      4
                      90%      4
                      95%      5
                      98%      7
                      99%     10
                     100%     27 (longest request)</code></pre>
                    <p>¬°Ahora llegamos casi a 3000 req/s! Nuestro servidor est√° aprovechando mejor el tiempo porque, cuando una petici√≥n est√° ocupada con algo, responde a la siguiente. ¬øSignifica esto que estamos listos para servir 2999 req/s? En absoluto; significa que, para esta petici√≥n en concreto, en local y con un n√∫mero limitado de peticiones, nuestro servidor responde a estas peticiones por segundo. Esta medici√≥n es s√≥lo un punto de partida que tenemos que refinar usando peticiones m√°s complejas y condiciones m√°s realistas.</p>
                    <p>Pod√©is ver m√°s detalles sobre c√≥mo usar ab en el <a href="http://codehero.co/como-hacer-pruebas-de-carga-servidores-web/">excelente tutorial de Jonathan Wiesel</a>. Tambi√©n se muestra c√≥mo usar Siege, otra herramienta similar.</p>
                    <h3 id="loadtest.js">loadtest.js</h3>
                    <p>Al principio del art√≠culo os he contado que √≠bamos a dejar node.js tranquilo. Pues lo siento pero os he soltado una mentirijilla. ¬øEn serio pens√°bais que √≠bamos a pasarnos una entrada entera sin hablar de nuestro servidor favorito?</p>
                    <p>El buen ingeniero, como el artesano de anta√±o, siempre elige sus herramientas cuidadosamente; y si no existen se las inventa. La siguiente librer√≠a es obra del autor de este art√≠culo; <a href="https://npmjs.org/package/loadtest">loadtest</a> (<a href="https://github.com/alexfernandez/loadtest">repo GitHub</a>). Aparte de usar los par√°metros m√°s importantes compatibles con ab, como <code>-n</code> o <code>-c</code>, tiene una opci√≥n muy interesante que nos permite enviar un n√∫mero fijo de peticiones por segundo a nuestro servidor.</p>
                    <p>Para instalarla s√≥lo tenemos que usar <a href="https://npmjs.org/">npm</a>, el gestor de paquetes de node, como root:</p>
                    <pre><code># npm install -g loadtest</code></pre>
                    <p>En Ubuntu o Mac OS X se usa <code>sudo</code>:</p>
                    <pre><code>$ sudo npm install -g loadtest</code></pre>
                    <p>¬°Y listo! Ya tenemos instalado un comando <code>loadtest</code> que nos va a ayudar a probar nuestra aplicaci√≥n web. Un ejemplo sencillo, similar a los que ya hemos visto. Vamos a lanzar 10000 peticiones, con 10 peticiones concurrentes, a un servidor local en el puerto 8080. Las opciones b√°sicas son iguales que las de <code>ab</code>; la salida es similar pero m√°s escueta, y los resultados se parecen bastante:</p>
                    <pre><code>$ loadtest -c 10 -n 10000 http://127.0.0.1:8080/

                    Completed requests:  10000
                    Total time:          4.472977 s
                    Requests per second: 2236
                    Mean latency:        4.44 ms

                    Percentage of the requests served within a certain time
                      50%      4 ms
                      90%      6 ms
                      95%      7 ms
                      99%      12 ms
                     100%      19 ms (longest request)</code></pre>
                    <p>Ahora en lugar de 3000 estamos alrededor de las 2200 req/s. ¬øPor qu√© nos quedamos bastante por debajo que antes? Hay que decir que loadtest tiene un cierto overhead al estar escrito para node.js y no en C s√∫per-optimizado, por lo que es normal ver cifras ligeramente peores. Dicho esto, la herramienta est√° optimizada y el overhead no deber√≠a nunca superar un milisegundo. Aunque en nuestro servidor trivial se note mucho, en un servidor m√°s realista (o lanzando las pruebas desde otra m√°quina) deber√≠a dejar de afectar a los resultados.</p>
                    <p>Ahora empieza la diversi√≥n. ¬øPor qu√© usar loadtest si tiene los mismos par√°metros y peor rendimiento? Por las opciones avanzadas, claro.</p>
                    <ul>
                    <li>‚Äìrps <em>tasa</em>: n√∫mero de peticiones por segundo a lanzar. ¬øNo es igual que la concurrencia? En absoluto; <code>--rps</code> lanza (siempre que pueda) la tasa pedida de peticiones, aunque las anteriores no hayan terminado. Esto nos permite controlar la carga de peticiones que enviamos cada segundo al servidor.</li>
                    <li>‚Äìnoagent: no usar el agente integrado de node.js. Por defecto, node.js limita la salida a 10 conexiones en vuelo, lo que puede afectar a las pruebas; este par√°metro elimina ese l√≠mite. Viene activado de f√°brica.</li>
                    <li>‚Äìkeepalive: usa el agente <a href="https://github.com/TBEDP/agentkeepalive">agentkeepalive</a>, que a√±ade <code>Connection: Keep-alive</code> y est√° muy mejorado sobre el que viene en node.js. Por defecto, node.js limita la salida a 10 conexiones en vuelo, con lo que las pruebas pueden verse afectadas.</li>
                    </ul>
                    <p><em>Nota de precauci√≥n</em>: el agente por defecto de node.js apesta de mala manera: tiene un pool de s√≥lo 10 conexiones, lo que nos puede limitar si necesitamos enviar m√°s peticiones a la vez. Pero desactivarlo con <code>--default</code> implica no usar agente y perder el keep-alive: se abre una conexi√≥n cada vez al servidor que estamos probando, lo que puede perjudicar el rendimiento. Es recomendable usar siempre <code>--keepalive</code>.</p>
                    <p>Vamos a probar ahora nuestro servidor con agentkeepalive:</p>
                    <pre><code>$ loadtest --keepalive -c 10 -n 10000 http://127.0.0.1:8080/

                    Completed requests:  10000
                    Total time:          3.04171 s
                    Requests per second: 3288
                    Mean latency:        3.01 ms

                    Percentage of the requests served within a certain time
                      50%      2 ms
                      90%      4 ms
                      95%      5 ms
                      99%      9 ms
                     100%      22 ms (longest request)</code></pre>
                    <p>¬°Mejor resultado incluso que con <code>ab</code>! Al no tener que abrir una conexi√≥n nueva para cada petici√≥n, ahorramos recursos tanto en el cliente de pruebas como en el servidor.</p>
                    <p>De nuevo, ¬øsignifica eso que nuestro servidor est√° preparado para responder a 3000 req/s? Vamos a verificarlo enchuf√°ndole un n√∫mero fijo de peticiones por segundo con la opci√≥n <code>--rps</code>, a ver qu√© pasa. Empezaremos con 2000:</p>
                    <pre><code>$ loadtest --keepalive --rps 2000 -n 10000 http://127.0.0.1:8080/

                    Completed requests:  10000
                    Total time:          5.545158 s
                    Requests per second: 1803
                    Mean latency:        137.73 ms

                    Percentage of the requests served within a certain time
                      50%      146 ms
                      90%      214 ms
                      95%      221 ms
                      99%      232 ms
                     100%      238 ms (longest request)</code></pre>
                    <p>¬°Sorpresa! Nuestro servidor se viene abajo, ahora no llega ni a las 2000 req/s pedidas. ¬øC√≥mo puede ser que d√© peor respuesta (y una latencia de m√°s de 100 milisegundos) con menos peticiones? Para explicar este misterio tenemos que ver c√≥mo funcionan estas pruebas. Con <code>ab</code> (y con loadtest hasta ahora) enviamos unas cuantas peticiones al servidor; el servidor las procesa como puede, con los retrasos que tenga en cada momento, y cuando las responde enviamos m√°s. El par√°metro -c se asegura de que nunca haya m√°s de 10 peticiones en vuelo. Siempre podemos subir la concurrencia, pero tampoco ser√° muy realista acumular decenas de peticiones en vuelo.</p>
                    <p>Con loadtest podemos usar una tasa fija de req/s, que le llegar√°n al servidor tanto si ha respondido las anteriores como si no. Puede ser que otro proceso est√© usando la CPU en ese momento, o que tenga que acceder a disco para recuperar un recurso. Resultado: en cuanto se le empiezan a acumular peticiones, la respuesta decae. Esta prueba es mucho m√°s realista que las anteriores, y evidencia m√°s c√≥mo el servidor puede sufrir incluso ante pausas de unos pocos milisegundos. En la vida real las peticiones tienden a agolparse todav√≠a m√°s, en lugar de llegar siempre de forma uniforme, as√≠ que la respuesta ser√° incluso peor.</p>
                    <p>Ahora vamos a probar a enviarle m√°s peticiones al servidor con las mismas req/s, para comprobar si la respuesta es constante. Si vemos que empieza a venirse abajo, o que la respuesta es muy err√°tica, sabremos que nuestro servidor no aguanta la carga. loadtest muestra una traza cada cinco segundos que nos va indicando c√≥mo va el proceso, as√≠ que vamos a lanzarle 100K peticiones en total.</p>
                    <pre><code>$ loadtest --keepalive --rps 2000 -n 100000 http://127.0.0.1:8080/

                    Requests: 8949 (9%), requests per second: 1790, mean latency: 64.12 ms
                    Requests: 18728 (19%), requests per second: 1956, mean latency: 47.21 ms
                    Requests: 27418 (27%), requests per second: 1738, mean latency: 579.05 ms
                    Requests: 36508 (37%), requests per second: 1818, mean latency: 1112.56 ms
                    Requests: 45402 (45%), requests per second: 1778, mean latency: 1670.62 ms
                    Requests: 55122 (55%), requests per second: 1944, mean latency: 1986.79 ms
                    Requests: 65180 (65%), requests per second: 2011, mean latency: 1954.7 ms
                    Requests: 75231 (75%), requests per second: 2011, mean latency: 1954.87 ms
                    Requests: 84895 (85%), requests per second: 1933, mean latency: 2016.92 ms
                    Requests: 93252 (93%), requests per second: 1671, mean latency: 2464.68 ms

                    Completed requests:  100000
                    Total time:          53.96305 s
                    Requests per second: 1853
                    Mean latency:        1512.71 ms</code></pre>
                    <p>Aunque la CPU no llegue al 100%, vemos c√≥mo se van acumulando peticiones y la respuesta se degrada. Bajando el flujo a 1500 req/s los resultados son estables.</p>
                    <pre><code>$ loadtest --keepalive --rps 1500 -n 100000 http://127.0.0.1:8080/

                    Requests: 6060 (6%), requests per second: 1212, mean latency: 6.6 ms
                    Requests: 13545 (14%), requests per second: 1497, mean latency: 8.41 ms
                    Requests: 21059 (21%), requests per second: 1503, mean latency: 5.83 ms
                    Requests: 28564 (29%), requests per second: 1501, mean latency: 2.69 ms
                    Requests: 36060 (36%), requests per second: 1499, mean latency: 3.32 ms
                    Requests: 43564 (44%), requests per second: 1501, mean latency: 2.66 ms
                    Requests: 51062 (51%), requests per second: 1500, mean latency: 2.27 ms
                    Requests: 58565 (59%), requests per second: 1501, mean latency: 1.7 ms
                    Requests: 66059 (66%), requests per second: 1499, mean latency: 2.97 ms
                    Requests: 73563 (74%), requests per second: 1501, mean latency: 1.71 ms
                    Requests: 80991 (81%), requests per second: 1486, mean latency: 13.12 ms
                    Requests: 88565 (89%), requests per second: 1515, mean latency: 13.02 ms
                    Requests: 96059 (96%), requests per second: 1499, mean latency: 9.44 ms

                    Completed requests:  100000
                    Total time:          67.630739 s
                    Requests per second: 1479
                    Mean latency:        5.63 ms</code></pre>
                    <p>Ahora s√≠: hay una cierta oscilaci√≥n alrededor del 66% de las peticiones, pero el sistema tiende a recuperarse y no es inestable como antes.</p>
                    <h2 id="dise√±o-de-pruebas-de-carga">Dise√±o de pruebas de carga</h2>
                    <p>Ya sabemos c√≥mo lanzar nuestra herramienta favorita. Ahora, ¬°paremos el carro un poco! Vamos a ver para qu√© sirven estas pruebas de carga, y c√≥mo dise√±ar unas pruebas que cumplan nuestras expectativas.</p>
                    <h3 id="preguntas-preguntas">Preguntas, preguntas</h3>
                    <p>El objetivo de las pruebas de carga es responder a preguntas vitales para cualquier negocio:</p>
                    <ul>
                    <li>¬øSoportar√°n los servidores la carga esperada?</li>
                    <li>¬øCu√°ntos servidores necesitaremos?</li>
                    <li>¬øCu√°l ser√° el coste por usuario?</li>
                    </ul>
                    <p>Pero, por importantes que puedan parecer para quien las hace, estas preguntas son s√≥lo el principio del camino; necesitamos algo m√°s concreto para empezar a movernos.</p>
                    <p>Las pruebas bien dise√±adas se basan siempre en preguntas <em>precisas y objetivas</em>: que se responden con datos y no con opiniones, y que no admiten ambig√ºedad (por ejemplo, se pueden responder con ‚Äús√≠‚Äù o ‚Äúno‚Äù). Tambi√©n tienen que ser <em>realistas</em>: basadas en nuestra situaci√≥n y no en alg√∫n objetivo inalcanzable. La primera parte de nuestro trabajo es reformular las preguntas de negocio a su equivalente ingenieril. Por ejemplo:</p>
                    <ul>
                    <li><em>¬øDar√° buen rendimiento nuestro servidor?</em> Esta pregunta es algo difusa: es dif√≠cil saber qu√© es ‚Äúbueno‚Äù sin precisar m√°s. Tenemos que pulirla algo m√°s.</li>
                    <li><em>¬øResponder√° el servidor a las peticiones en menos de 100 ms?</em> Hemos avanzado bastante: esta pregunta es bastante m√°s precisa y no tiene ambig√ºedades. Pero representa un objetivo inalcanzable, porque siempre puede haber alguna petici√≥n que se escape del margen pedido (por buena que sea la media), y por tanto la respuesta ser√° casi siempre ‚Äúno‚Äù. Tenemos que afinar todav√≠a m√°s.</li>
                    <li>¬øResponder√° el servidor al 99% de las peticiones en menos de 100 ms? Ahora s√≠: podemos responder claramente con datos, y es alcanzable.</li>
                    </ul>
                    <h3 id="objetivos">Objetivos</h3>
                    <p>Aunque hemos avanzado algo, ni siquiera hemos terminado con la primera pregunta de negocio, que (recordemos) era:</p>
                    <blockquote>
                    <p>¬øSoportar√°n los servidores la carga esperada?</p>
                    </blockquote>
                    <p>Todav√≠a nos queda traducir ‚Äúcarga esperada‚Äù a cifras concretas. Por ejemplo, supongamos que esperamos llegar a 100.000 usuarios √∫nicos en el primer a√±o: ¬øcu√°ntas peticiones veremos en nuestros servidores? ¬øCu√°ntas p√°ginas servidas, y cu√°ntas peticiones a nuestra API son eso?</p>
                    <p>El camino tradicional es el siguiente: partiendo de una cifra imaginaria de usuarios, podemos estimar cu√°ntos usuarios activos esperar por d√≠a, cu√°ntas peticiones por minuto hace un usuario activo, y ya s√≥lo necesitamos saber las peticiones por minuto que aguanta nuestro servidor para calcular cu√°ntos usuarios √∫nicos podemos tener contentos con un √∫nico servidor. Pero tiene un peque√±o problema: cada estimaci√≥n que hagamos a√±ade <em>incertidumbre</em> a nuestros resultados. Una ecuaci√≥n con tantos par√°metros inciertos como la que estamos montando terminar√° dando pura fantas√≠a‚Ä¶</p>
                    <p>As√≠ que lo ideal es recoger primero datos reales sobre el terreno que soporten nuestros c√°lculos. Por ejemplo, si sabemos que la carga con 10.000 usuarios √∫nicos al mes es de 150 peticiones/minuto, es f√°cil estimar que con 100.000 tendremos 1500 peticiones/minuto.</p>
                    <p>S√≥lo nos falta saber cu√°ntas peticiones por minuto aguanta nuestro servidor, que es donde entran nuestras herramientas favoritas.</p>
                    <h3 id="limitaciones">Limitaciones</h3>
                    <p>Por definici√≥n, las pruebas de carga son una simulaci√≥n. Hay que ser consciente siempre del valor que tienen: nos permiten encontrar puntos flacos en nuestro sistema, y estimar el rendimiento real. Pero la realidad es muy tozuda, y por m√°s que queramos reproducirla en nuestro laboratorio, el mundo exterior nos va a dar sorpresas.</p>
                    <p>Es importante recalcar este punto cuando presentemos nuestros resultados al cliente o a la gente de negocio: a no ser que dediquemos tiempo infinito a replicar el tr√°fico real con todas sus peculiaridades, nuestras respuestas siempre ser√°n parciales y sujetas a error. Aunque pongamos todo nuestro esfuerzo puede que ni conozcamos todas estas peculiaridades en el momento de realizar las pruebas de carga, por lo que siempre nos podemos equivocar en nuestros objetivos.</p>
                    <p><em>M√°s consejos del abuelo cebolleta</em>: no afines demasiado calculando la carga que soporta cada m√°quina, date siempre margen para incluir las posibles incertidumbres. Pero: no uses ese margen para cubrir ineficiencias del sistema, lo necesitar√°s.</p>
                    <h3 id="bancos-de-pruebas-benchmarks">Bancos de pruebas (<em>benchmarks</em>)</h3>
                    <p>Para estimar el rendimiento de un servidor podemos usar los bancos de pruebas de los fabricantes: para cualquier producto que queramos usar es f√°cil encontrar informes de cu√°ntas peticiones por segundo puede responder, tiempos de respuesta y dem√°s. Estos <em>benchmarks</em> son muy populares sobre todo para bases de datos, aunque en los √∫ltimos tiempos muchos fabricantes han prohibido su publicaci√≥n. Pero sigue habiendo mucha informaci√≥n para software libre. As√≠ pues, ¬øpor qu√© complicarse la vida haciendo pruebas propias?</p>
                    <p>Lo malo es que los <em>benchmarks</em> son como el <a href="http://es.wikipedia.org/wiki/Or%C3%A1culo_de_Delfos">or√°culo de Delfos</a>: le podemos preguntar lo que queramos, que nos responder√° lo que le d√© la gana. El fabricante siempre intentar√° que su producto salga bien parado en las pruebas, y adem√°s probablemente sepa optimizarlo mejor que nosotros. Por poner un ejemplo: no responde igual un servidor Apache cuando sirve una imagen est√°tica de 5 KB que cuando sirve una p√°gina PHP de 200 KB. Adem√°s, la respuesta puede cambiar much√≠simo seg√∫n la configuraci√≥n que usemos.</p>
                    <p>Por eso se realizan pruebas de carga simulando tr√°fico real: lanzando peticiones t√≠picas al servidor y midiendo la respuesta en condiciones similares a las reales. Todo centrado en nuestro caso concreto y lo m√°s realista posible. No es muy √∫til simular peticiones de 100 bytes si el grueso de nuestro tr√°fico son p√°ginas de 100 KB ‚Äî aunque, como veremos, estas pruebas tienen su sitio para detectar problemas.</p>
                    <h2 id="escalabilidad">Escalabilidad</h2>
                    <p>Hemos dise√±ado las pruebas y las hemos lanzado. ¬øQu√© hacemos ahora con los resultados? Afinar y mejorar, claro.</p>
                    <p>En esta siguiente etapa vamos a intentar dar respuesta a la segunda pregunta de negocio que vimos arriba:</p>
                    <blockquote>
                    <p>¬øCu√°ntos servidores necesitaremos?</p>
                    </blockquote>
                    <p>Y de forma indirecta a la tercera, porque podremos calcular el coste por usuario.</p>
                    <p>Si un servidor nos da 100 req/s, para conseguir llegar a 2000 req/s vamos a necesitar 20 servidores, ¬øverdad? ¬°En tus sue√±os!</p>
                    <h3 id="un-camino-espinoso">Un camino espinoso</h3>
                    <p>El camino hacia la escalabilidad pasa siempre por mantener la (<em>atenci√≥n, palabro</em>) linealidad.</p>
                    <p>Suena rimbombante, pero lo que queremos decir se expresa de forma muy sencilla: doblando el n√∫mero de servidores soportaremos el doble de carga, expresada en peticiones por segundo. Y lo mismo duplicando el n√∫mero de n√∫cleos, la frecuencia de la CPU, etc√©tera. Tambi√©n necesitaremos el doble de memoria y de recursos en general.</p>
                    <p>Lo que nos permiten las pruebas de carga es, precisamente, detectar p√©rdidas de linealidad. Supongamos que con un solo proceso aguantamos 15 req/s. ¬øQu√© podemos esperar si activamos el <a href="modo-cluster">modo cluster</a> con dos procesadores? Pues 30 req/s, claro‚Ä¶ Si vemos menos, es que hemos encontrado un cuello de botella. Puede ser que se nos haya agotado alg√∫n otro recurso (memoria, conexiones, o lo que sea) o que otro elemento est√© saturado (base de datos, interfaz de red‚Ä¶).</p>
                    <p>Tambi√©n nos permitir√°n encontrar problemas de rendimiento cuando la respuesta no sea constante en el tiempo y tenga altibajos, o cuando el sistema empiece a dar errores al saturarse. Lo ideal es que el sistema deje de responder a las peticiones que no pueda atender, pero siga sirviendo correctamente un n√∫mero razonable.</p>
                    <p>Los fallos deben tambi√©n ser (atenci√≥n, semi-palabro) recuperables: el sistema debe recuperarse por s√≠ solo, sin necesidad de tener que reiniciar servicios ni servidores. Cualquier error que deje tumbados nuestros servidores es un problema catastr√≥fico que tenemos que resolver ‚Äî o nos quitar√° el sue√±o por las noches. Muchas veces literalmente, en forma de llamada airada de responsables de servicio o (incluso peor) de usuarios.</p>
                    <h3 id="reduccionismo">Reduccionismo</h3>
                    <p>Supongamos que tenemos un problema de rendimiento que afecta a nuestros servidores, pero no sabemos d√≥nde. Lo que tenemos que hacer para encontrarlo es ir quitando elementos que puedan afectar al rendimiento. ¬øSospechamos que la base de datos puede estar muy saturada? Usaremos una base de datos limpia para pruebas. ¬øTodav√≠a sospechamos que se satura? La sustituiremos por una respuesta simulada, o haremos pruebas de carga de la base de datos sola. ¬øLa memoria es un problema? Usaremos objetos vac√≠os que no ocupen memoria.</p>
                    <p>As√≠ iremos quitando trozos de nuestro sistema, corriendo las pruebas de nuevo, hasta que el problema desaparezca. En ese momento habremos encontrado la causa.</p>
                    <h2 id="cu√°ndo-probar">Cu√°ndo probar</h2>
                    <p>¬øCu√°ndo debemos lanzar pruebas de carga, y con qu√© frecuencia?</p>
                    <h3 id="pruebas-reactivas-o-proactivas">Pruebas reactivas o proactivas</h3>
                    <p>Lo habitual es usar las pruebas de carga para diagnosticar un problema en el sistema. Usando nuestro m√©todo reduccionista, podemos encontrar f√°cilmente la causa de un problema.</p>
                    <p>Para nota, podemos ejecutar pruebas de carga antes de ver los problemas. Un buen momento es, por ejemplo, cuando esperemos un pico de carga, un lanzamiento, etc√©tera. Tambi√©n es recomendable usar las pruebas de carga para dimensionamiento del sistema: saber, por ejemplo, cu√°ntos servidores necesitaremos. Siempre con las precauciones que hemos visto arriba sobre la precisi√≥n de los resultados, dejando margen para posibles incertidumbres.</p>
                    <h3 id="pruebas-integradas">Pruebas integradas</h3>
                    <p>Un truco que vale la pena aplicar: podemos lanzar una versi√≥n reducida de nuestras pruebas de carga de forma rutinaria, junto con el resto de pruebas de sistema. As√≠ podremos detectar regresiones antes de ponerlas en producci√≥n.</p>
                    <p>Por ejemplo, podemos lanzar 1000 peticiones seguidas, verificar que no dan errores y que la latencia media est√° por debajo de 100 ms. Si alguna de estas condiciones falla abortamos el despliegue.</p>
                    <p>loadtest tiene una API que nos permite lanzarlo desde nuestro programa node.js y recoger los resultados en una callback. Es trivial integrarlo en nuestro sistema de despliegue autom√°tico ‚Äî cosa sobre la que hablaremos otro d√≠a.</p>
                    <h2 id="conclusi√≥n">Conclusi√≥n</h2>
                    <p>Eso es todo por hoy. Si no conoc√≠as las pruebas de carga, espero haberte ayudado a a√±adir una flecha a tu carcaj. En cualquier caso, usa esta t√©cnica cuando sea necesario, aprende a conocer tus herramientas, y sobre todo ¬°disfruta!</p>
								</div>
							</section>
							<section id="last">
								<div class="container">
									<p>
									Original publicado en <a href="http://www.godtic.com/blog/2013/08/27/pruebas-de-carga/">GodTIC</a> el 2013-08-27.
									</p>
									<p>
									Back to the <a href="/">index</a>.
									</p>
								</div>
							</section>

				<!-- Footer -->
					<section id="footer">
						<div class="container">
							<ul class="copyright">
								<li>
									¬© <a href="mailto:alexfernandeznpm@gmail.com">Alex Fern√°ndez</a>.
									<a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a>.
								</li>
								<li>Original design: <a href="http://html5up.net">HTML5 UP</a></li>
							</ul>
						</div>
					</section>

			</div>
	</body>
</html>
